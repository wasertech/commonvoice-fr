FROM cnstark/pytorch:1.13.0-py3.9.12-cuda11.7.1-ubuntu20.04

ARG transcorer_repo=wasertech/TranScorerLM
ARG transcorer_branch=33e945d5f421d55a2925d9bc87d4766beb22c37c
# ARG transformers_repo=huggingface/transformers
# ARG transformers_branch=04ab5605fbb4ef207b10bf2772d88c53fc242e83
# ARG datasets_repo=huggingface/datasets
# ARG datasets_branch=3b16e08dd599f4646a77a5ca88b6445467e1e7e9

# Model parameters
ARG base_model_name=""
ENV BASE_MODEL_NAME=$base_model_name

ARG model_language=fr
ENV MODEL_LANGUAGE=$model_language

ARG disable_no_ignore_characters_warning=0
ENV DONT_WARN_IGNORE_CHARS=$disable_no_ignore_characters_warning

# Training hyper-parameters
ARG train_batch_size=16
ENV TRAIN_BATCH_SIZE=$batch_size

ARG eval_batch_size=8
ENV EVAL_BATCH_SIZE=$batch_size

ARG opm_nproc=1
ENV OMP_NUM_THREADS=$opm_nproc

ARG epochs=7
ENV EPOCHS=$epochs

ARG learning_rate="3e-4"
ENV LEARNING_RATE=$learning_rate

ARG dropout=0.0
ENV DROPOUT=$dropout

ARG amp=1
ENV AMP=$amp

ARG freeze_encoder=1
ENV FREEZE_ENCODER=$freeze_encoder

# Gradient Accumulation Steps
ARG gas=2
ENV GAS=$gas

ARG gradien_checkpointing=1
ENV GRAD_CHECK=$gradien_checkpointing

# Warm up uses ratio over steps
# Set WARMUP_RATIO=0 to use steps instead
ARG warm_up_steps=500
ENV WARMUP_STEPS=$warm_up_steps

ARG warm_up_ratio=0.1
ENV WARMUP_RATIO=$warm_up_ratio

ARG save_steps=400
ENV SAVE_STEPS=$save_steps

ARG eval_steps=100
ENV EVAL_STEPS=$eval_steps

ARG eval_strategy="steps"
ENV EVAL_STRAT=$eval_strategy

ARG max_amount_checkpoints=3
ENV MAX_CHECKPOINTS=$max_amount_checkpoints

# Data processing workers
# (too many might OOM) recommended at 1 worker per CPU core or lower.
# Meaning if you have 12 cores (24 threads) and 100 Gb of RAM, set it to 12 workers.
# You could probably handle more but with 1 worker per thread,
# you are likely to experience OOM issues.
ARG nproc=12
ENV NPROC=$nproc

ARG hub_token=""
ENV HUB_API_TOKEN=$hub_token

ARG uid=999
ENV UID=$uid

ARG gid=999
ENV GID=$gid

# Make sure we can extract filenames with UTF-8 chars
ENV LANG=C.UTF-8

# Avoid keyboard-configuration step
ENV DEBIAN_FRONTEND noninteractive

ENV HOMEDIR /home/trainer

ENV VIRTUAL_ENV_NAME w2v-train
ENV VIRTUAL_ENV $HOMEDIR/$VIRTUAL_ENV_NAME

ENV TRANSCORER_DIR $HOMEDIR/transcorer
ENV TRANSCORER_BRANCH=$transcorer_branch

ENV PATH="$VIRTUAL_ENV/bin:$PATH"

RUN env

# Get basic packages
RUN apt-get -qq update && apt-get -qq install -y --no-install-recommends \
    build-essential \
    curl \
    wget \
    git \
    ffmpeg \
    python3 \
    python3-pip \
    ca-certificates \
    cmake \
    libboost-all-dev \
    zlib1g-dev \
    libbz2-dev \
    liblzma-dev \
    pkg-config \
    g++ \
    virtualenv \
    unzip \
    pixz \
    sox \
    sudo \
    libsox-fmt-all \
    locales locales-all \
    xz-utils

# For uploading models to HuggingFace hub
RUN apt -qq install -y --no-install-recommends git-lfs

# For uptodate libsndfile>1
RUN sudo apt -qq install -y --no-install-recommends software-properties-common

RUN sudo add-apt-repository "deb http://archive.ubuntu.com/ubuntu/ jammy main universe"

COPY etc/apt/preferences.d/libsndfile /etc/apt/preferences.d/libsndfile

RUN sudo apt -qq update && sudo apt -qq install -y libsndfile-dev

RUN groupadd -g $GID trainer && \
    adduser --system --uid $UID --group trainer

RUN echo "trainer ALL=(root) NOPASSWD:ALL" > /etc/sudoers.d/trainer && \
    chmod 0440 /etc/sudoers.d/trainer

# Below that point, nothing requires being root
USER trainer

WORKDIR $HOMEDIR

RUN virtualenv --python=/usr/bin/python3 $VIRTUAL_ENV_NAME

ENV PATH=$HOMEDIR/$VIRTUAL_ENV_NAME/bin:$PATH

RUN git clone https://github.com/$transcorer_repo.git $TRANSCORER_DIR

WORKDIR $TRANSCORER_DIR

RUN git checkout $transcorer_branch

RUN pip install torch 

RUN pip install numpy

RUN pip install torchaudio

RUN pip install datasets>=1.18.0

RUN pip install evaluate

RUN pip install transformers>=4.26.1

RUN pip install -e .

WORKDIR $HOMEDIR

RUN pip install librosa

RUN pip install jiwer

RUN pip install -U numba

WORKDIR $HOMEDIR

#ENV PATH="$HOMEDIR/kenlm/build/bin/:$PATH"

# Copy now so that docker build can leverage caches
COPY --chown=trainer:trainer . $HOMEDIR/

COPY --chown=trainer:trainer ${MODEL_LANGUAGE}/ $HOMEDIR/${MODEL_LANGUAGE}/

ENTRYPOINT "$HOMEDIR/run.sh"
